{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dye3qyJKJlEW"
   },
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grv8-We-JlEZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F37i9KdfJlEa",
    "outputId": "5b0e5107-7071-4916-bd4f-3240c450af61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6nQ_0esDJlEa"
   },
   "source": [
    "# Hyperparametes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7G2JGMXZJlEa"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters and Configuration\n",
    "config = {\n",
    "    # Dataset parameters\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 2,\n",
    "    \"num_classes\": 100,\n",
    "    # Model parameters\n",
    "    \"model_type\": \"resnet50\",\n",
    "    \"pretrained\": True,\n",
    "    # Training parameters\n",
    "    \"epochs\": 30,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"adam\",  # options: 'adam', 'sgd'\n",
    "    # Learning rate scheduler\n",
    "    \"scheduler\": \"plateau\",  # options: 'plateau', 'step', 'cosine'\n",
    "    \"scheduler_patience\": 3,\n",
    "    \"scheduler_factor\": 0.1,\n",
    "    # Regularization\n",
    "    \"weight_decay\": 1e-4,\n",
    "    # Data augmentation parameters\n",
    "    \"crop_padding\": 4,\n",
    "    \"normalize_mean\": (0.5071, 0.4867, 0.4408),\n",
    "    \"normalize_std\": (0.2675, 0.2565, 0.2761),\n",
    "    # Device\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    # Random seed for reproducibility\n",
    "    \"seed\": 42,\n",
    "    # Save directory\n",
    "    \"save_dir\": \"./checkpoints\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvpXnsR9JlEb"
   },
   "source": [
    "## Downloading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WU_iM9d7JlEb"
   },
   "outputs": [],
   "source": [
    "# Transformation functions\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=config[\"crop_padding\"]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(config[\"normalize_mean\"], config[\"normalize_std\"]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(config[\"normalize_mean\"], config[\"normalize_std\"]),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTtWxBB_JlEc",
    "outputId": "018a5ecd-6855-4b5e-c770-b46b784f5e6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Download data\n",
    "train_dataset = datasets.CIFAR100(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "test_dataset = datasets.CIFAR100(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Load data\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "# get class names for training\n",
    "class_names: list[str] = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNVBW62vJlEc"
   },
   "source": [
    "# Resnet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fw4mTnfHJlEc"
   },
   "outputs": [],
   "source": [
    "def get_resnet_model(config):\n",
    "    \"\"\"Load pretrained ResNet and modify for CIFAR100\"\"\"\n",
    "    if config[\"model_type\"] == \"resnet50\":\n",
    "        model = models.resnet50(pretrained=config[\"pretrained\"])\n",
    "    elif config[\"model_type\"] == \"resnet18\":\n",
    "        model = models.resnet18(pretrained=config[\"pretrained\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {config['model_type']}\")\n",
    "\n",
    "    # Modify the first conv layer to handle CIFAR100's 32x32 images\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()  # Remove maxpool as we have smaller images\n",
    "\n",
    "    # Modify final fully connected layer for specified number of classes\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, config[\"num_classes\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_optimizer(model, config):\n",
    "    \"\"\"Get optimizer based on configuration\"\"\"\n",
    "    if config[\"optimizer\"] == \"adam\":\n",
    "        return optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=config[\"learning_rate\"],\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "        )\n",
    "    elif config[\"optimizer\"] == \"sgd\":\n",
    "        return optim.SGD(\n",
    "            model.parameters(),\n",
    "            lr=config[\"learning_rate\"],\n",
    "            momentum=0.9,\n",
    "            weight_decay=config[\"weight_decay\"],\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {config['optimizer']}\")\n",
    "\n",
    "\n",
    "def get_scheduler(optimizer, config):\n",
    "    \"\"\"Get learning rate scheduler based on configuration\"\"\"\n",
    "    if config[\"scheduler\"] == \"plateau\":\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            patience=config[\"scheduler_patience\"],\n",
    "            factor=config[\"scheduler_factor\"],\n",
    "            verbose=True,\n",
    "        )\n",
    "    elif config[\"scheduler\"] == \"step\":\n",
    "        return optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=10, gamma=config[\"scheduler_factor\"]\n",
    "        )\n",
    "    elif config[\"scheduler\"] == \"cosine\":\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config[\"epochs\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler: {config['scheduler']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "grug7fSkJlEd"
   },
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rLNTgvyJlEd",
    "outputId": "9c692a02-05ee-4954-86c6-157027bd50f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home1/suyunov/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home1/suyunov/.local/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home1/suyunov/.local/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 782/782 [00:44<00:00, 17.68it/s, Loss=3.24, Acc=20.1] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.5027, Test Acc: 33.26%\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory ./checkpoints does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 167\u001b[0m\n\u001b[1;32m    164\u001b[0m     writer\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 120\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m     best_acc \u001b[38;5;241m=\u001b[39m test_acc\n\u001b[1;32m    119\u001b[0m     best_preds, best_labels \u001b[38;5;241m=\u001b[39m all_preds, all_labels \u001b[38;5;66;03m# Save predictions and labels\u001b[39;00m\n\u001b[0;32m--> 120\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moptimizer_state_dict\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_acc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_acc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msave_dir\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/best_model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;66;03m# Adjust learning rate\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplateau\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:716\u001b[0m, in \u001b[0;36m_open_zipfile_writer\u001b[0;34m(name_or_buffer)\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    715\u001b[0m     container \u001b[38;5;241m=\u001b[39m _open_zipfile_writer_buffer\n\u001b[0;32m--> 716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/torch/serialization.py:687\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__init__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39mPyTorchFileWriter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_stream))\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 687\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Parent directory ./checkpoints does not exist."
     ]
    }
   ],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, config):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=\"Training\")\n",
    "    for inputs, targets in pbar:\n",
    "        inputs, targets = inputs.to(config[\"device\"]), targets.to(config[\"device\"])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "        pbar.set_postfix(\n",
    "            {\"Loss\": running_loss / len(train_loader), \"Acc\": 100.0 * correct / total}\n",
    "        )\n",
    "\n",
    "    return running_loss / len(train_loader), 100.0 * correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, config):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(config[\"device\"]), targets.to(config[\"device\"])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            # Store all predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    return (\n",
    "        running_loss / len(test_loader),\n",
    "        100.0 * correct / total,\n",
    "        all_preds,\n",
    "        all_labels,\n",
    "    )\n",
    "\n",
    "\n",
    "def main():\n",
    "    writer = SummaryWriter(log_dir=\"./runs\")\n",
    "    # Load data\n",
    "    train_loader = DataLoader(\n",
    "        datasets.CIFAR100(\n",
    "            root=\"./data\", train=True, download=True, transform=transform_train\n",
    "        ),\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=True,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        datasets.CIFAR100(\n",
    "            root=\"./data\", train=False, download=True, transform=transform_test\n",
    "        ),\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        shuffle=False,\n",
    "        num_workers=config[\"num_workers\"],\n",
    "    )\n",
    "\n",
    "    # Create a TensorBoard SummaryWriter and Display dataset images data\n",
    "    # writer = SummaryWriter()\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "    img_grid = torchvision.utils.make_grid(images)\n",
    "    writer.add_image(\"Sixty Four CIFAR100 Images\", img_grid)\n",
    "\n",
    "    # Initialize model\n",
    "    model = get_resnet_model(config)\n",
    "    model = model.to(config[\"device\"])\n",
    "\n",
    "    # Add model graph to TensorBoard\n",
    "    writer.add_graph(model, images.to(config[\"device\"]))\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = get_optimizer(model, config)\n",
    "    scheduler = get_scheduler(optimizer, config)\n",
    "\n",
    "    # Training loop\n",
    "    best_acc = 0\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [], []\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "\n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, config\n",
    "        )\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "\n",
    "        # Evaluate\n",
    "        test_loss, test_acc, all_preds, all_labels = evaluate(\n",
    "            model, test_loader, criterion, config\n",
    "        )\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "\n",
    "        # Tensorboard\n",
    "        writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/train\", train_acc, epoch)\n",
    "        writer.add_scalar(\"Loss/test\", test_loss, epoch)\n",
    "        writer.add_scalar(\"Accuracy/test\", test_acc, epoch)\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "\n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            best_preds, best_labels = (\n",
    "                all_preds,\n",
    "                all_labels,\n",
    "            )  # Save predictions and labels\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                    \"best_acc\": best_acc,\n",
    "                    \"config\": config,\n",
    "                },\n",
    "                f\"{config['save_dir']}/best_model.pth\",\n",
    "            )\n",
    "\n",
    "        # Adjust learning rate\n",
    "        if config[\"scheduler\"] == \"plateau\":\n",
    "            scheduler.step(test_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=\"Train\")\n",
    "    plt.plot(test_losses, label=\"Test\")\n",
    "    plt.title(\"Loss vs Epoch\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label=\"Train\")\n",
    "    plt.plot(test_accs, label=\"Test\")\n",
    "    plt.title(\"Accuracy vs Epoch\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # confusion matrix\n",
    "    print(\"Plotting confusion matrix for the best model\")\n",
    "    cm = confusion_matrix(best_labels, best_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Close the TensorBoard SummaryWriter\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikvYLjVJJlEd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
