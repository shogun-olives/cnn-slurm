{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms,datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters and Configuration\n",
    "config = {\n",
    "    # Dataset parameters\n",
    "    'batch_size': 64,\n",
    "    'num_workers': 2,\n",
    "    'num_classes': 100,\n",
    "    \n",
    "    # Model parameters\n",
    "    'model_type': 'resnet50',\n",
    "    'pretrained': True,\n",
    "    \n",
    "    # Training parameters\n",
    "    'epochs': 30,\n",
    "    'learning_rate': 0.001,\n",
    "    'optimizer': 'adam',  # options: 'adam', 'sgd'\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    'scheduler': 'plateau',  # options: 'plateau', 'step', 'cosine'\n",
    "    'scheduler_patience': 3,\n",
    "    'scheduler_factor': 0.1,\n",
    "    \n",
    "    # Regularization\n",
    "    'weight_decay': 1e-4,\n",
    "    \n",
    "    # Data augmentation parameters\n",
    "    'crop_padding': 4,\n",
    "    'normalize_mean': (0.5071, 0.4867, 0.4408),\n",
    "    'normalize_std': (0.2675, 0.2565, 0.2761),\n",
    "    \n",
    "    # Device\n",
    "    'device': torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \n",
    "    # Random seed for reproducibility\n",
    "    'seed': 42,\n",
    "    \n",
    "    # Save directory\n",
    "    'save_dir': './checkpoints'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation functions\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=config['crop_padding']),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(config['normalize_mean'], config['normalize_std']),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(config['normalize_mean'], config['normalize_std']),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "batch_size = 64\n",
    "\n",
    "# Download data\n",
    "train_dataset = datasets.CIFAR100(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "test_dataset = datasets.CIFAR100(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "\n",
    "# Load data\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "# get class names for training\n",
    "class_names: list[str] = train_dataset.classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_resnet_model(config):\n",
    "    \"\"\"Load pretrained ResNet and modify for CIFAR100\"\"\"\n",
    "    if config['model_type'] == 'resnet50':\n",
    "        model = models.resnet50(pretrained=config['pretrained'])\n",
    "    elif config['model_type'] == 'resnet18':\n",
    "        model = models.resnet18(pretrained=config['pretrained'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {config['model_type']}\")\n",
    "    \n",
    "    # Modify the first conv layer to handle CIFAR100's 32x32 images\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "    model.maxpool = nn.Identity()  # Remove maxpool as we have smaller images\n",
    "    \n",
    "    # Modify final fully connected layer for specified number of classes\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, config['num_classes'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def get_optimizer(model, config):\n",
    "    \"\"\"Get optimizer based on configuration\"\"\"\n",
    "    if config['optimizer'] == 'adam':\n",
    "        return optim.Adam(model.parameters(), \n",
    "                         lr=config['learning_rate'],\n",
    "                         weight_decay=config['weight_decay'])\n",
    "    elif config['optimizer'] == 'sgd':\n",
    "        return optim.SGD(model.parameters(),\n",
    "                        lr=config['learning_rate'],\n",
    "                        momentum=0.9,\n",
    "                        weight_decay=config['weight_decay'])\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported optimizer: {config['optimizer']}\")\n",
    "\n",
    "def get_scheduler(optimizer, config):\n",
    "    \"\"\"Get learning rate scheduler based on configuration\"\"\"\n",
    "    if config['scheduler'] == 'plateau':\n",
    "        return optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            patience=config['scheduler_patience'],\n",
    "            factor=config['scheduler_factor'],\n",
    "            verbose=True\n",
    "        )\n",
    "    elif config['scheduler'] == 'step':\n",
    "        return optim.lr_scheduler.StepLR(\n",
    "            optimizer,\n",
    "            step_size=10,\n",
    "            gamma=config['scheduler_factor']\n",
    "        )\n",
    "    elif config['scheduler'] == 'cosine':\n",
    "        return optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=config['epochs']\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported scheduler: {config['scheduler']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, config):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc='Training')\n",
    "    for inputs, targets in pbar:\n",
    "        inputs, targets = inputs.to(config['device']), targets.to(config['device'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'Loss': running_loss/len(train_loader),\n",
    "                         'Acc': 100.*correct/total})\n",
    "    \n",
    "    return running_loss/len(train_loader), 100.*correct/total\n",
    "\n",
    "def evaluate(model, test_loader, criterion, config):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(config['device']), targets.to(config['device'])\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    return running_loss/len(test_loader), 100.*correct/total\n",
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    train_loader = DataLoader(\n",
    "        datasets.CIFAR100(root=\"./data\", train=True, download=True, transform=transform_train),\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers']\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        datasets.CIFAR100(root=\"./data\", train=False, download=True, transform=transform_test),\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers']\n",
    "    )\n",
    "\n",
    "    # Initialize model\n",
    "    model = get_resnet_model(config)\n",
    "    model = model.to(config['device'])\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = get_optimizer(model, config)\n",
    "    scheduler = get_scheduler(optimizer, config)\n",
    "    \n",
    "    # Training loop\n",
    "    best_acc = 0\n",
    "    train_losses, train_accs = [], []\n",
    "    test_losses, test_accs = [], []\n",
    "    \n",
    "    for epoch in range(config['epochs']):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, config)\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        \n",
    "        # Evaluate\n",
    "        test_loss, test_acc = evaluate(model, test_loader, criterion, config)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accs.append(test_acc)\n",
    "        \n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_acc': best_acc,\n",
    "                'config': config\n",
    "            }, f\"{config['save_dir']}/best_model.pth\")\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        if config['scheduler'] == 'plateau':\n",
    "            scheduler.step(test_loss)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "    \n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label='Train')\n",
    "    plt.plot(test_losses, label='Test')\n",
    "    plt.title('Loss vs Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(train_accs, label='Train')\n",
    "    plt.plot(test_accs, label='Test')\n",
    "    plt.title('Accuracy vs Epoch')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
